---
title: "Pexlib Extraction Demo"
output: html_notebook
---

```{r Initialization, message=FALSE, warning=FALSE}
library(reticulate)
library(dplyr)
library(lubridate)
library(sparklyr)

source("src/helper.R")

pexlib <- import("pexlib")

req <- import("requests")
req$packages$urllib3$disable_warnings() 
rm(req)

```



```{r Multi-tags}
client = pexlib$Client(
  "https://casagszwebpi1.corp.riotinto.org/piwebapi/",
  "CORP\\crda.courtoisie",
  "#:3EmSY/4wKDfRBurD"                # Change me!
)

sc <- ConnectToSpark()
sdf_sql(sc, "USE reduction")
sdf_sql(sc, "SET hive.exec.dynamic.partition.mode = nonstrict")

tryCatch({
  
  # Extract tag daat from its string construction
  plant_tags <- readLines("data/points-AAR.txt") %>%
    data.frame(path = .) %>%
    transmute(path,
              tag = right(path, nchar(path) - 13),
              plant = substr(tag, 1, 3),
              pot = ifelse(right(tag, 7) == "IMFC.PV",
                           substr(tag, 15, 18),
                           ifelse(substr(tag, 11, 11) == "A", 
                                  paste0("1", substr(tag, 12, 14)), 
                                  paste0("2", substr(tag, 12, 14)))),
              kind = ifelse(right(tag, 8) == "VCUVE.PV", "v",
                            ifelse(right(tag, 7) == "IMFC.PV", "i",
                                   paste0("iano", substr(tag,22, 23)))))
  
  # We will use that table more than once, lets save it in a variable.
  plant_ianode_1s <- tbl(sc, "reduction.aar_ianode_1s")
  
  # Get the most recent time we have data for each pots
  print("Get the most recent timestamp for each pot.")
  plant_max_ts <- plant_ianode_1s %>%
    select(pot, year, month) %>%
    group_by(pot, year) %>%
    summarise(month = max(month, na.rm = T)) %>%
    slice_max(year) %>%
    inner_join(plant_ianode_1s, by = c("pot", "year", "month")) %>%
    select(pot, ts) %>%
    group_by(pot) %>%
    summarize(max_ts = max(ts, na.rm = T)) %>%
    arrange(pot) %>%
    collect()
  
  # Create intervals for extraction
  extraction_intervals <- plant_max_ts %>%
    inner_join(plant_tags, by = "pot") %>%
    mutate(start_time = max_ts + dseconds(1),
           end_time = start_time + ddays(1),
           sync_time = start_time) %>%
    select(-max_ts) %>%
    arrange(pot, tag)

    
  client$open()

  extraction_intervals %>%
    group_by(pot) %>%
    group_walk(function(.x, .y) {
      i <<- 0
      pot <- .y$pot
      pot_tags <- as.data.frame(.x)
      tag_count <- nrow(pot_tags)
      rownames(pot_tags) <- pot_tags$path
      
      pot_data <- lapply(pot_tags$path, function(tag_path) {
        row <- pot_tags[tag_path,]
        i <<- i + 1
        print(paste0("[", i, "/", tag_count, "] ", tag_path))
        pexlib$extractor$extract_dataframe(
          client,
          tag_path,
          start_time = as.character(row$start_time),
          end_time   = as.character(row$end_time),
          sync_time  = as.character(row$sync_time),
          interval   = "1s",
          stream     = "Recorded")
      }) %>%
        bind_rows() %>%
        select(-strval)
        

      print(paste0("Pivoting pot ", pot, "."))
      pivoted_pot_data <- pot_data %>% 
        inner_join(plant_tags, by = "tag") %>% 
        transmute(pot, 
                  year = year(ts), 
                  month = month(ts), 
                  ts = floor_date(ts, "seconds"), 
                  kind, 
                  val = numval) %>% 
        pivot_wider(id_cols = c("pot", "year", "month", "ts"), 
                    names_from = kind,
                    values_from =  val,
                    values_fn = first) %>%
        select(ts,
               iano01, iano02, iano03, iano04, iano05, iano06, iano07, iano08, iano09, iano10, iano11,
               iano12, iano13, iano14, iano15, iano16, iano17, iano18, iano19, iano20, iano21, iano22,
               iano23, iano24, i, v, pot, year, month)
      

      print(paste("Saving", pot))
      sdf_pivoted_pot_data <- copy_to(sc, 
                                      df = pivoted_pot_data, 
                                      name = "pivoted_pot_data", 
                                      overwrite = T)
      
      
      tryCatch({
        sdf_pivoted_pot_data %>%
          spark_write_table("zzz_test", mode = "append")
      },
      error = {
        sdf_pivoted_pot_data %>%
          spark_write_table("zzz_test",
                            partition_by = c("pot", "year", "month"),
                            mode = "overwrite")
      })
        
        
    })  
  
  
  



  

  
  
  # 
  # 
  # 
  #   
  # aar_data <- extracted_data %>%
  #   filter(substr(tag, 1, 3) == "AAR") %>%
  #   transmute(ts = floor_date(ts, unit = "seconds"),
  #             kind = ifelse(right(tag, 8) == "VCUVE.PV", "v",
  #                           ifelse(right(tag, 7) == "IMFC.PV", "i",
  #                                  paste0("iano", substr(tag,22, 23)))),
  #             val = numval,
  #             pot = ifelse(right(tag, 7) == "IMFC.PV",
  #                          substr(tag, 15, 18),
  #                          ifelse(substr(tag, 11, 11) == "A", 
  #                                 paste0("1", substr(tag, 12, 14)), 
  #                                 paste0("2", substr(tag, 12, 14)))),
  #             year = year(ts),
  #             month = month(ts)) %>%
  #   sdf_pivot(pot + year + month + ts ~ kind, fun.aggregate = first) %>%
  #   select(ts,
  #          iano01, iano02, iano03, iano04, iano05, iano06, iano07, iano08, iano09, iano10, iano11,
  #          iano12, iano13, iano14, iano15, iano16, iano17, iano18, iano19, iano20, iano21, iano22,
  #          iano23, iano24, i, v, pot, year, month)
  # 
  # spark_write_table(aar_data, "aar_ianode_1s", 
  #                   partition_by = c("pot", "year", "month"),
  #                   mode = "overwrite")
  #    
  #         
  # alm_data <- extracted_data %>%
  #   filter(substr(tag, 1, 3) == "ALM") %>%
  #   transmute(ts = floor_date(ts, unit = "seconds"),
  #             kind = ifelse(right(tag, 8) == "VCUVE.PV", "v",
  #                           ifelse(right(tag, 7) == "IMFC.PV", "i",
  #                                  paste0("iano", substr(tag,23, 24)))),
  #             val = numval,
  #             pot = ifelse(right(tag, 7) == "IMFC.PV",
  #                          substr(tag, 16, 19),
  #                          substr(tag, 12, 15)),
  #             year = year(ts),
  #             month = month(ts)) %>%
  #   sdf_pivot(pot + year + month + ts ~ kind, fun.aggregate = first) %>%
  #   select(ts,
  #          iano01, iano02, iano03, iano04, iano05, iano06, iano07, iano08, iano09, iano10, iano11,
  #          iano12, iano13, iano14, iano15, iano16, iano17, iano18, iano19, iano20,
  #          i, v, pot, year, month)
  # 
  #   spark_write_table(alm_data, "alm_ianode_1s", 
  #                   partition_by = c("pot", "year", "month"),
  #                   mode = "overwrite")
    
  
  
},
finally = {
  client$close()
  spark_disconnect(sc)
  spark_disconnect_all()
})

```


Rpartition
```{r}
# sdf_sql(sc, "USE reduction")
# 
# sdf_sql(sc, "SET hive.merge.mapfiles = true")
# sdf_sql(sc, "SET hive.merge.mapredfiles = true")
# sdf_sql(sc, "SET hive.merge.size.per.task = 256000000")
# sdf_sql(sc, "SET hive.merge.smallfiles.avgsize = 134217728")
# sdf_sql(sc, "SET hive.exec.compress.output = true")
# sdf_sql(sc, "SET hive.exec.dynamic.partition.mode = nonstrict")
# sdf_sql(sc, "SET parquet.compression = snappy")
# sdf_sql(sc, "CREATE TABLE reduction.aar_ianode_1s_2 LIKE reduction.aar_ianode_1s")
# 
# spark_read_table(sc, "aar_ianode_1s") %>%
#   spark_write_table("aar_ianode_1s_2", 
#                     partition_by = c("pot", "year", "month"),
#                     mode = "overwrite")
# 
# sdf_sql(sc, "ALTER TABLE aar_ianode_1s RENAME TO aar_ianode_1s_tmp")
# sdf_sql(sc, "ALTER TABLE aar_ianode_1s_2 RENAME TO aar_ianode_1s")
# sdf_sql(sc, "DROP TABLE aar_ianode_1s_tmp")

```



